{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DEPENDENCIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gdown\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DATASET**\n",
    "\n",
    "**Data Set Description:**\n",
    "The task is to predict whether a citizen is happy to live in a city based on certain parameters of the city as rated by the citizens on a scale of 1-5 during a survey.\n",
    "\n",
    "**Attribute Information:**\n",
    "\n",
    "- **D (Decision/Class Attribute):**  \n",
    "  - Values: 0 (unhappy) and 1 (happy)  \n",
    "  - Column 1 of the file\n",
    "- **X1 (Availability of Information about City Services):**  \n",
    "  - Values: 1 to 5  \n",
    "  - Column 2 of the file\n",
    "- **X2 (Cost of Housing):**  \n",
    "  - Values: 1 to 5\n",
    "- **X3 (Overall Quality of Public Schools):**  \n",
    "  - Values: 1 to 5\n",
    "- **X4 (Trust in the Local Police):**  \n",
    "  - Values: 1 to 5\n",
    "- **X5 (Maintenance of Streets and Sidewalks):**  \n",
    "  - Values: 1 to 5\n",
    "- **X6 (Availability of Social Community Events):**  \n",
    "  - Values: 1 to 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdown.download(\n",
    "    \"https://drive.google.com/uc?id=1QiwDpdCitGx9MWvB29Kc4LuifFBAO6OJ\",\n",
    "    \"Xtr.txt\",\n",
    ")\n",
    "gdown.download(\n",
    "    \"https://drive.google.com/uc?id=1aRKIeM2B2ZBmn-tKcylfyb6BYFHT6wwt\",\n",
    "    \"Xte.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   D  X1  X2  X3  X4  X5  X6\n",
      "0  0   3   3   3   4   2   4\n",
      "1  0   3   2   3   5   4   3\n",
      "2  1   5   3   3   3   3   5\n",
      "3  0   5   4   3   3   3   5\n",
      "4  0   5   4   3   3   3   5\n",
      "   D  X1  X2  X3  X4  X5  X6\n",
      "0  0   5   1   4   4   4   5\n",
      "1  0   5   2   2   4   4   5\n",
      "2  0   5   3   5   4   5   5\n",
      "3  1   3   4   4   5   1   3\n",
      "4  1   5   1   5   5   5   5\n"
     ]
    }
   ],
   "source": [
    "Xtr = pd.read_csv(\"Xtr.txt\", header=None).values[:, 0]\n",
    "Xte = pd.read_csv(\"Xte.txt\", header=None).values[:, 0]\n",
    "Xtr = np.array(list(map(lambda x: x.split(\",\"), Xtr)))\n",
    "Xte = np.array(list(map(lambda x: x.split(\",\"), Xte)))\n",
    "Xtr = pd.DataFrame(Xtr[1:].astype(int), columns=Xtr[0])\n",
    "Xte = pd.DataFrame(Xte[1:].astype(int), columns=Xte[0])\n",
    "print(Xtr.head())\n",
    "print(Xte.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BAYES CLASSIFIER**\n",
    "\n",
    "**Bayes' Theorem**:\n",
    "Bayes' theorem is the foundation of the Bayes Classifier. It's used to calculate the probability of a class (in this case, a category) given a set of features. The formula is as follows:\n",
    "\n",
    "$ P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)} $\n",
    "\n",
    "Where:\n",
    "- $P(C|X)$ is the posterior probability of class $C$ given the features $X$.\n",
    "- $P(X|C)$ is the likelihood, which is the probability of observing the features $X$ given class $C$.\n",
    "- $P(C)$ is the prior probability of class $C$.\n",
    "- $P(X)$ is the probability of observing the features $X$ regardless of the class.\n",
    "\n",
    "**Laplacian Smoothing (Additive Smoothing)**:\n",
    "Laplacian smoothing is applied to address issues with zero probabilities, which can occur when certain feature-class combinations have no training data. The formula for Laplacian smoothing is as follows:\n",
    "\n",
    "$ P(X|C) = \\frac{N_{X,C} + \\alpha}{N_C + \\alpha \\cdot N_{\\text{features}}}$\n",
    "\n",
    "Where:\n",
    "- $N_{X,C}$ is the count of times feature $X$ appears in class $C$.\n",
    "- $N_C$ is the count of samples in class $C$.\n",
    "- $\\alpha$ is the smoothing parameter (usually set to 1).\n",
    "- $N_{\\text{features}}$ is the total number of features.\n",
    "\n",
    "**Bayes Classifier Implementation**:\n",
    "\n",
    "1. **Initialization**: The class takes the training data $X$ and corresponding labels $y$. It also allows setting the Laplacian smoothing parameter $\\alpha$. The class calculates class priors and feature likelihoods during initialization.\n",
    "\n",
    "2. **Priors**: The `calculate_prior` method computes the prior probabilities of each class, which is the probability of a data point belonging to a specific class based on the training data.\n",
    "\n",
    "3. **Likelihoods**: The `calculate_likelihood` method calculates the likelihoods of each feature for each class, considering Laplacian smoothing. It calculates how often each feature appears in each class.\n",
    "\n",
    "4. **Prediction**: The `predict` method takes a new data point $X$ and calculates the posterior probability for each class. It uses the likelihoods and priors to make predictions.\n",
    "\n",
    "5. **Scoring**: The `score` method evaluates the classifier's accuracy by comparing its predictions with actual labels.\n",
    "\n",
    "In practice, Laplacian smoothing helps ensure that even if certain feature-class combinations are absent in the training data, they won't result in zero probabilities, improving the classifier's robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesClassifier:\n",
    "    def __init__(self, X, y, alpha=1, scaler=None):\n",
    "        if scaler is not None:\n",
    "            self.scaler = scaler\n",
    "            X = self.scaler.fit_transform(X)\n",
    "        else:\n",
    "            self.scaler = None\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.alpha = alpha\n",
    "        self.classes = np.unique(y)\n",
    "        self.n_classes = len(self.classes)\n",
    "        self.n_features = X.shape[1]\n",
    "        self.priors = self.calculate_prior()\n",
    "        self.likelihoods = self.calculate_likelihood()\n",
    "\n",
    "    def calculate_prior(self):\n",
    "        priors = np.zeros(self.n_classes)\n",
    "        for i, c in enumerate(self.classes):\n",
    "            priors[i] = np.mean(self.y == c)\n",
    "        return priors\n",
    "\n",
    "    def calculate_likelihood(self):\n",
    "        likelihoods = np.zeros((self.n_classes, self.n_features))\n",
    "        for i, c in enumerate(self.classes):\n",
    "            # Laplacian smoothing \n",
    "            likelihoods[i, :] = (np.sum(self.X[self.y == c], axis=0) + self.alpha) / (\n",
    "                np.sum(self.y == c) + self.alpha * 2\n",
    "            )\n",
    "        return likelihoods\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.scaler is not None:\n",
    "            X = self.scaler.transform(X)\n",
    "        y_pred = np.zeros(len(X))\n",
    "        for i, x in enumerate(X):\n",
    "            posteriors = np.zeros(self.n_classes)\n",
    "            for j, c in enumerate(self.classes):\n",
    "                likelihood = np.prod(self.likelihoods[j, :][x == 1]) * np.prod(\n",
    "                    1 - self.likelihoods[j, :][x == 0]\n",
    "                )\n",
    "                posteriors[j] = likelihood * self.priors[j]\n",
    "            y_pred[i] = self.classes[np.argmax(posteriors)]\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score without scaling (Tr): 0.4418604651162791\n",
      "Score with scaling (Tr): 0.5426356589147286\n",
      "Score without scaling (Te): 0.5\n",
      "Score with scaling (Te): 0.5\n"
     ]
    }
   ],
   "source": [
    "clf_ns = BayesClassifier(\n",
    "    Xtr.iloc[:, 1:], Xtr.iloc[:, 0], scaler=None,\n",
    ")\n",
    "clf_ws = BayesClassifier(\n",
    "    Xtr.iloc[:, 1:], Xtr.iloc[:, 0], scaler=StandardScaler(),\n",
    ")\n",
    "print(\"Score without scaling (Tr):\", clf_ns.score(Xtr.iloc[:, 1:], Xtr.iloc[:, 0]))\n",
    "print(\"Score with scaling (Tr):\", clf_ws.score(Xtr.iloc[:, 1:], Xtr.iloc[:, 0]))\n",
    "print(\"Score without scaling (Te):\", clf_ns.score(Xte.iloc[:, 1:], Xte.iloc[:, 0]))\n",
    "print(\"Score with scaling (Te):\", clf_ws.score(Xte.iloc[:, 1:], Xte.iloc[:, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing with sklearn.naive_bayes.GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.5426356589147286\n",
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "clf_sk = BernoulliNB(alpha=1)\n",
    "clf_sk.fit(Xtr.iloc[:, 1:], Xtr.iloc[:, 0])\n",
    "print(\"Training accuracy:\", clf_sk.score(Xtr.iloc[:, 1:], Xtr.iloc[:, 0]))\n",
    "print(\"Test accuracy:\", clf_sk.score(Xte.iloc[:, 1:], Xte.iloc[:, 0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
